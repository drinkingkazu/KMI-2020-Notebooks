{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelized Batch Data Loading\n",
    "\n",
    "In this notebook we practice a Pytorch API to prepare a _batch data_ for machine learning algorithm training.\n",
    "\n",
    "## Goals\n",
    "* Learn what `DataLoader` is and how to create\n",
    "* Learn how to stream batch data using `DataLoader`\n",
    "\n",
    "First, we download a python package for the workshop and make sure it's up-to-date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "![ -d kmi ] || git clone https://github.com/drinkingkazu/kmi\n",
    "! cd kmi && git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... followed by importing some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib   # for plotting in 2D\n",
    "%matplotlib inline  \n",
    "import torch, numpy # only to set random seed (used by underlying python packages)\n",
    "import numpy as np  # only to set random seed (used by underlying python packages)\n",
    "# Set random seed\n",
    "SEED=123\n",
    "_=numpy.random.seed(SEED)\n",
    "_=torch.manual_seed(SEED)\n",
    "# Add the path for the python package downloaded\n",
    "import sys\n",
    "sys.path.insert(0, 'kmi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DataLoader`\n",
    "So whaat is it? When we train a machine learning algorithm using a stochastic gradient descent, we want to form a random subset of data, callled a _batch_, to compute the error and weight updates. Recall how our `Dataset` works: it tells you the total data size + gives an intuitive access to a data by a data index integer. A pytorch `DataLoader` takes `Dataset` instance with those features and automatize the packaging of batch data. \n",
    "\n",
    "Also we want this process to be fast such that we maximize the compute time spent for the algorithm forward and gradient calculations. `DataLoader` implements a scheme of multiple parallel workers to perform multiple batch data preparation to realize this.\n",
    "\n",
    "### Instantiation of `DataLoader`\n",
    "The instantiation is trivial through pytorch API, but here we will use another factory function from `kmi` package. The function takes a configuration parameters in the form of a python dictionary just like `Dataset`. In fact, you give both the `Dataset` and `DataLoader` configuration in a single dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmi.iotools import loader_factory\n",
    "\n",
    "BATCH_SIZE=100\n",
    "\n",
    "# Let's start with Dataset configuration\n",
    "dataset_config = dict(data_files = ['/scratch/workshop_2020_01/train/proton.h5'],\n",
    "                      dataset = 'DenseImage2D',\n",
    "                      angles  = True\n",
    "                     )\n",
    "\n",
    "# Next, DataLoader configuration\n",
    "loader_config = dict( batch_size = BATCH_SIZE,\n",
    "                      collate = 'DenseCollate',\n",
    "                      shuffle = False,\n",
    "                      num_workers = 4,\n",
    "                    )\n",
    "\n",
    "# Combine two configurations (or you could make one dict to begin with)\n",
    "loader_config.update(dataset_config)\n",
    "\n",
    "# Create a loader\n",
    "loader = loader_factory(**loader_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tables\n",
    "OUT_FNAME='aho.h5'\n",
    "\n",
    "FILTERS = tables.Filters(complib='zlib', complevel=5)\n",
    "fout = tables.open_file(OUT_FNAME,mode='w', filters=FILTERS)\n",
    "data_storage = fout.create_carray(fout.root,'data',tables.Float32Atom(),shape=[100000,192,192])\n",
    "label_storage = fout.create_carray(fout.root,'pdg',tables.Int32Atom(),shape=[100000])\n",
    "\n",
    "for i in range(100000 / BATCH_SIZE):\n",
    "    \n",
    "label_storage[:] = label\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
